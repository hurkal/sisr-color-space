{"cells":[{"cell_type":"markdown","metadata":{"id":"a-Yp3Y595-xX"},"source":["# YAPILACAKLAR\n","\n","* Edge detected görselleri diff görsel ile karşılaştır.\n","* En iyi (uyumlu) edge detection bulmaya çalış\n","* SREdgeNet araştırılabilir\n","* SRGAN çalışmasında PSNR ve SSIM karşılaştırmaları neden Y kanalı üzerinde gerçekleştiriliyor?\n","* DAHA DA ÖNEMLİSİ: Tüm renk kanalları üzerinde aynı anda PSNR ve SSIM hesaplamak güvenilir mi? Çünkü her renk kanalının kendi içinde max. ve min. değerleri farklı, oysa PSNR formülünde önemli max. value kullanılıyor, bu önemli bir detay! PSNR hesaplaması RGB için bir farka sebep olmazken YCC ve CIELAB renk uzaylarında önemli bir fark yaratabilir. Bunun da karşılaştırmalı sonuçlar bir yayın olabilir.\n","* INTERPOLASYON yöntemleri harici bir metot olarak kullanılabilmeli\n","\n","* clip() ile maximum değeri aşan INTERPOLASYON yöntemlerinde limitleme yapıldı. - clip() iptal edildi ve Normalizasyon işlemi uygulandı.\n","* Tüm renk uzaylarının olası renk adetlerini listele, az çeşide indirgenen renk uzaylarında başarının kediliğinden yükseldiği hipotezini doğrulamaya çalış\n","* normalize_values() FONKSİYONUNDAKİ NOTU OKU!!!\n","* normalize_factor hep 255 olarak alınıyor. Diğer renk uzaylarının eksenlerine göre değişiklik göstermesi gerekmez mi?\n","* Fark hesaplamasında elde edilen fark rgb2gray() ile dönüştürülüyor, ancak CMYK ile de işlem yapıyoruz. Bunun düzeltilmesi gerekli!\n","* modified_dir neden yapıldı?"]},{"cell_type":"markdown","metadata":{"id":"zTjDI0FNV32q"},"source":["# ÇÖZÜNÜRLÜK İYİLEŞTİRME"]},{"cell_type":"markdown","metadata":{"id":"nY5Yfr_0xtRo"},"source":["## Tanımlamalar"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1618,"status":"ok","timestamp":1735569845224,"user":{"displayName":"Sugg Esso","userId":"12124034271188103324"},"user_tz":-180},"id":"vdHJMpCG4inO","outputId":"1be377c2-bfed-4e8c-b2cd-940b75387811"},"outputs":[{"name":"stdout","output_type":"stream","text":["URBAN100 is default dataset.\n"]}],"source":["enhancement_mode=False\n","edge_detection_mode=False\n","show_comparison_results=True\n","# Results of outputs' modified versions, output_modified_dir[] will be used instead of\n","# output_dir[]\n","modified_results=False\n","\n","import cv2\n","from enum import Enum\n","\n","\n","class Conversions(Enum):\n","  NONE = 0\n","  RGB2YCRCB = 1\n","  RGB2LAB = 2\n","  #RGB2CMYK = 3\n","  #BGR2YCRCB = 2\n","\n","class Methods(Enum):\n","  INTERPOLATION_NN = 0\n","  INTERPOLATION_LINEAR =1\n","  INTERPOLATION_CUBIC =2\n","  INTERPOLATION_LANCZOS =3\n","  SRGAN = 4\n","  REAL_ESRGAN = 5\n","  SWIN2SR = 6\n","\n","class Dataset():\n","  def __init__(self, name, path, lr_dir, hr_dir):\n","    self.name = name\n","    self.path = path\n","    self.lr_dir = path + lr_dir +\"/\"\n","    self.hr_dir = path + hr_dir + \"/\"\n","\n","DATASET_LIST = {}\n","DATASET_LIST[\"Set5\"] = Dataset(\"Set5\",\"/content/drive/MyDrive/Colab Notebooks/SRGAN/dataset/Set5/\",\"LRbicx4\",\"GTmod12\")\n","DATASET_LIST[\"Set14\"] = Dataset(\"Set14\",\"/content/drive/MyDrive/Colab Notebooks/SRGAN/dataset/Set14/\",\"LRbicx4\",\"GTmod12\")\n","DATASET_LIST[\"BSDS100\"] = Dataset(\"BSDS100\",\"/content/drive/MyDrive/Colab Notebooks/SRGAN/dataset/BSDS100/\",\"LR\",\"HR\")\n","DATASET_LIST[\"URBAN100\"] = Dataset(\"URBAN100\",\"/content/drive/MyDrive/Colab Notebooks/SRGAN/dataset/URBAN100/\",\"LR\",\"HR\")\n","\n","\n","#Use this:\n","dataset = DATASET_LIST[\"URBAN100\"]\n","print(dataset.name +\" is default dataset.\")\n","\n","input_dir = dataset.lr_dir\n","input_lab_dir = input_dir + \"lab/\"\n","input_edge_dir = input_dir + \"edge/\"\n","ground_truth_dir = dataset.hr_dir\n","\n","base_dir={}\n","output_dir={}\n","output_diff_dir={}\n","output_modified_dir={}\n","\n","\n","base_dir[Methods.INTERPOLATION_NN]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/0-Interpolations/NN/\"\n","output_dir[Methods.INTERPOLATION_NN]=base_dir[Methods.INTERPOLATION_NN]+\"results/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.INTERPOLATION_NN]= output_dir[Methods.INTERPOLATION_NN] + \"diff/\"\n","\n","base_dir[Methods.INTERPOLATION_LINEAR]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/0-Interpolations/LINEAR/\"\n","output_dir[Methods.INTERPOLATION_LINEAR]=base_dir[Methods.INTERPOLATION_LINEAR]+\"results/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.INTERPOLATION_LINEAR]= output_dir[Methods.INTERPOLATION_LINEAR] + \"diff/\"\n","\n","base_dir[Methods.INTERPOLATION_CUBIC]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/0-Interpolations/CUBIC/\"\n","output_dir[Methods.INTERPOLATION_CUBIC]=base_dir[Methods.INTERPOLATION_CUBIC]+\"results/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.INTERPOLATION_CUBIC]= output_dir[Methods.INTERPOLATION_CUBIC] + \"diff/\"\n","\n","base_dir[Methods.INTERPOLATION_LANCZOS]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/0-Interpolations/LANCZOS/\"\n","output_dir[Methods.INTERPOLATION_LANCZOS]=base_dir[Methods.INTERPOLATION_LANCZOS]+\"results/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.INTERPOLATION_LANCZOS]= output_dir[Methods.INTERPOLATION_LANCZOS] + \"diff/\"\n","\n","base_dir[Methods.SRGAN]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/5-SRGAN-PyTorch-master/\"\n","output_dir[Methods.SRGAN]=base_dir[Methods.SRGAN]+\"results/test/v1/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.SRGAN]= output_dir[Methods.SRGAN] + \"diff/\"\n","output_modified_dir[Methods.SRGAN]=output_dir[Methods.SRGAN] + \"modified/\"\n","\n","base_dir[Methods.REAL_ESRGAN]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/6-Real-ESRGAN/\"\n","output_dir[Methods.REAL_ESRGAN]=base_dir[Methods.REAL_ESRGAN]+\"results/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.REAL_ESRGAN]= output_dir[Methods.REAL_ESRGAN] + \"diff/\"\n","\n","base_dir[Methods.SWIN2SR]=\"/content/drive/MyDrive/Colab Notebooks/SRGAN/9-Swin2SR/\"\n","output_dir[Methods.SWIN2SR]=base_dir[Methods.SWIN2SR]+\"results/swin2sr_classical_sr_x4/\"+dataset.name+\"/\"\n","output_diff_dir[Methods.SWIN2SR]= output_dir[Methods.SWIN2SR] + \"diff/\"\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1735569845224,"user":{"displayName":"Sugg Esso","userId":"12124034271188103324"},"user_tz":-180},"id":"Xa6VzyUUVPid"},"outputs":[],"source":["class ColorAxis():\n","  def __init__(self,name,min,max):\n","    self.name=name\n","    self.min=min\n","    self.max=max\n","\n","\n","class Color():\n","  def __init__(self, name, axes):\n","    self.name = name\n","    self.axes = axes\n","\n","COLORS = {}\n","COLORS[\"RGB\"] = Color(\"RGB\",[\n","    ColorAxis(\"R\",0,255),\n","    ColorAxis(\"G\",0,255),\n","    ColorAxis(\"B\",0,255)\n","])\n","\n","COLORS[\"YCC\"] = Color(\"YCC\",[\n","    ColorAxis(\"Y\",16,235),\n","    ColorAxis(\"Cb\",16,240),\n","    ColorAxis(\"Cr\",16,240)\n","])\n","\n","COLORS[\"LAB\"] = Color(\"LAB\",[\n","    ColorAxis(\"L\",0,100),\n","    ColorAxis(\"A\",-110,110),\n","    ColorAxis(\"B\",-110,110)\n","])\n","\n","COLORS[\"CMYK\"] = Color(\"CMYK\",[\n","    ColorAxis(\"C\",0,100),\n","    ColorAxis(\"M\",0,100),\n","    ColorAxis(\"Y\",0,100),\n","    ColorAxis(\"K\",0,100)\n","])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"i5mgKM7mw7QV"},"outputs":[],"source":["#@title Dosya Sistemine Erişim\n","\n","#https://github.com/Lornatang/SRGAN-PyTorch\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# Add Tools\n","import sys\n","#sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/Tools')\n","#from Compare import *\n","\n","import torch\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"0xnA5rFHbBRx"},"source":["## Araçlar"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kaoAYmvKbEhn"},"outputs":[],"source":["#@title Dosya Sistemi Yönetimi\n","from os import walk\n","\n","def get_files(directory):\n","  return next(walk(directory), (None, None, []))[2]  # [] if no file\n","\n","def create_directory_if_not_exists(directory):\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","def file_to_matrix(file_src,conversion,remove_alpha=True,normalize_factor=1.0):\n","  # Both of the same\n","\n","  #import matplotlib.image as mpimg\n","  #img = np.array(img_to_array(load_img(file_src))).astype(np.float32) / 255.0\n","\n","  #print(\"[file_to_matrix()]\"+file_src)\n","\n","  if (\".npy\" in file_src):\n","    img = np.load(file_src,allow_pickle=False)\n","    #img = img / normalize_factor\n","  else:\n","    #RGB format\n","    #img = cv2.imread(file_src, cv2.IMREAD_UNCHANGED).astype(np.float32) / normalize_factor\n","    img = cv2.imread(file_src, cv2.IMREAD_COLOR).astype(np.float32)  / normalize_factor\n","\n","    # BGR -\u003e RGB\n","    img = img[...,::-1]\n","\n","  #Removing alpha channel if exists\n","  if remove_alpha:\n","    img = remove_alpha_channel(img)\n","\n","  #show_stats(img)\n","  img,color=color_convert(img,conversion)\n","\n","  return img,color\n","\n","def remove_alpha_channel(img):\n","  img = img[:,:,0:3]\n","  return img"]},{"cell_type":"markdown","metadata":{"id":"RKi7sYSG4yDT"},"source":["### Renk Uzayı Dönüşümü"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soTo4hIBKfz2"},"outputs":[],"source":["def color_convert(img,conversion):\n","  if (conversion==Conversions.RGB2YCRCB):\n","    return rgb2ycrcb(img),COLORS[\"YCC\"]\n","  elif (conversion==Conversions.RGB2LAB):\n","    return rgb2lab(img),COLORS[\"LAB\"]\n","  elif (conversion == Conversions.NONE):\n","    return img,COLORS[\"RGB\"]\n","  elif (conversion==Conversions.RGB2CMYK):\n","    return rgb2cmyk(img),COLORS[\"CMYK\"]\n","  else:\n","    raise Exception(\"Böyle bir dönüşüm tanımlı değil: \"+conversion.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0yLSxmKEYUM"},"outputs":[],"source":["#@title RGB2GRAY\n","def rgb2gray(matrix):\n","    return np.dot(matrix[...,:3], [0.2989, 0.5870, 0.1140])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzTdLqugyFvj"},"outputs":[],"source":["#@title RGB2CMYK\n","\n","def rgb2cmyk_old(img):\n","  img = Image.fromarray(np.uint8(img*255)).convert('CMYK')\n","  img = np.asarray(img)\n","  #show_stats(img)\n","  return img\n","\n","\n","RGB_SCALE = 255\n","CMYK_SCALE = 100\n","\n","def rgb_to_cmyk(r, g, b):\n","  if (r, g, b) == (0, 0, 0):\n","    # black\n","    return 0, 0, 0, CMYK_SCALE\n","\n","  # rgb [0,255] -\u003e cmy [0,1]\n","  c = 1 - r / RGB_SCALE\n","  m = 1 - g / RGB_SCALE\n","  y = 1 - b / RGB_SCALE\n","\n","  # extract out k [0, 1]\n","  min_cmy = min(c, m, y)\n","  c = (c - min_cmy) / (1 - min_cmy)\n","  m = (m - min_cmy) / (1 - min_cmy)\n","  y = (y - min_cmy) / (1 - min_cmy)\n","  k = min_cmy\n","\n","  # rescale to the range [0,CMYK_SCALE]\n","  return c * CMYK_SCALE, m * CMYK_SCALE, y * CMYK_SCALE, k * CMYK_SCALE\n","\n","\n","# Daha iyi bir yaklaşım olabilir, doğruluğu test edilmedi.\n","def rgb2cmyk_check(img):\n","  from PIL import Image\n","  return Image.fromarray(img).convert('CMYK')\n","\n","def rgb2cmyk(img):\n","  # Create float\n","  #print(\"converting rgb 2 cmyk\")\n","  '''\n","  print(\"rgb\")\n","  show_stats(img)\n","  '''\n","  rgb = img\n","  bgr = rgb[...,::-1]\n","  '''\n","  print(\"bgr\")\n","  show_stats(bgr)\n","  '''\n","\n","  # Extract channels\n","  with np.errstate(invalid='ignore', divide='ignore'):\n","    K = 1 - np.max(bgr, axis=2)\n","    C = (1-bgr[...,2] - K)/(1-K)\n","    M = (1-bgr[...,1] - K)/(1-K)\n","    Y = (1-bgr[...,0] - K)/(1-K)\n","\n","  # Convert the input BGR image to CMYK colorspace\n","  CMYK = (np.dstack((C,M,Y,K)) * 255).astype(np.uint8)\n","  '''\n","  print(\"c\"*20)\n","  show_stats(CMYK[...,0])\n","  print(\"m\"*20)\n","  show_stats(CMYK[...,1])\n","  print(\"y\"*20)\n","  show_stats(CMYK[...,2])\n","  print(\"k\"*20)\n","  show_stats(CMYK[...,3])\n","  '''\n","  return CMYK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp_OwKshiwXs"},"outputs":[],"source":["#@title RGB2LAB\n","#https://stackoverflow.com/questions/40586276/unexpected-output-while-converting-rgb-image-to-lab-image\n","def bgr2lab(img):\n","  return cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","\n","def rgb2lab(img):\n","\n","  #print(\"rgb2lab() Before Conversion\")\n","  #show_stats(img)\n","  #show_stats(img)\n","  img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","  #print(\"rgb2lab() After Conversion\")\n","  #show_stats(img)\n","\n","  #https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54\n","  img = (img + [0, 128, 128]) / [100, 255, 255]\n","\n","  '''\n","  ax = img\n","  fig, ax = plt.subplots(1, 4, figsize = (18, 30))\n","  ax[0].imshow(img)\n","  ax[0].axis('off')\n","  ax[0].set_title('Lab')\n","  for i, col in enumerate(['L', 'a', 'b'], 1):\n","      cv2_imshow(img[:, :, i-1], ax=ax[i])\n","      ax[i].axis('off')\n","      ax[i].set_title(col)\n","  fig.show()\n","  '''\n","\n","  return img\n","\n","  #return cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","\n","def cielab_conversion_test_1(org):\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","  import numpy as np\n","  #org = np.float32(cv2.imread(input_dir+\"bird.png\", -1))/255.\n","  #print(org.shape)\n","  lab_image = cv2.cvtColor(org, cv2.COLOR_BGR2LAB)\n","  l, a, b = cv2.split(lab_image)\n","  l_scaled = np.uint8(255.*(l - l.min())/(l.max() - l.min()))\n","  a_scaled = np.uint8(255.*(a - a.min())/(a.max() - a.min()))\n","  b_scaled = np.uint8(255.*(b - b.min())/(b.max() - b.min()))\n","\n","  cv2_imshow(l)\n","  cv2_imshow(a)\n","  cv2_imshow(b)\n","\n","  cv2_imshow(l_scaled)\n","  cv2_imshow(a_scaled)\n","  cv2_imshow(b_scaled)\n","\n","def cielab_conversion_test_2():\n","  from skimage import io, color\n","  import matplotlib.pyplot as plt\n","\n","  org = io.imread(input_dir+\"bird.png\")\n","  lab_image = color.rgb2lab(org)\n","  l = lab_image[:, :, 0]\n","  a = lab_image[:, :, 1]\n","  b = lab_image[:, :, 2]\n","\n","  fig, ax = plt.subplots(1, 3)\n","  plt.set_cmap('gray')\n","\n","  ax[0].imshow(l)\n","  ax[1].imshow(a)\n","  ax[2].imshow(b)\n","\n","#cielab_conversion_test_1()\n","#cielab_conversion_test_2()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_Uhims9415K"},"outputs":[],"source":["# Genelleştir\n","def rgb2lab_batch():\n","  import numpy as np\n","  import cv2\n","\n","  create_directory_if_not_exists(input_lab_dir)\n","\n","  file_names = get_files(input_dir)\n","  for file_name in file_names:\n","    img = cv2.imread(input_dir + file_name)\n","    LAB = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","\n","    #cv2.imwrite('L.png', LAB[:,:,0])\n","    #cv2.imwrite('a.png', LAB[:,:,1])\n","    #cv2.imwrite('b.png', LAB[:,:,2])\n","\n","    # BGR = cv2.cvtColor(LAB, cv2.COLOR_LAB2BGR)\n","    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    result = cv2.imwrite(input_lab_dir + file_name, LAB)\n","    print(file_name + \" :\" + str(result))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kcbbha-0YFLb"},"outputs":[],"source":["#@title RGB2YCrCb and BGR2YCrCb\n","def rgb2ycrcb(img):\n","  return cv2.cvtColor(img, cv2.COLOR_RGB2YCR_CB)\n","\n","def bgr2ycrcb(img):\n","  return cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n","\n","def getYfromRGB(img):\n","  return rgb2ycrcb(img)[:,:,0]"]},{"cell_type":"markdown","metadata":{"id":"T3AlYnnu0t4E"},"source":["### Interpolasyon Yöntemleri"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYNikRl8577q"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from math import sqrt, floor, ceil\n","from PIL import Image\n","import math\n","\n","\n","def read_image(path):\n","    '''Read image and return the image propertis.\n","    Parameters:\n","    path (string): Image path\n","    Returns:\n","    numpy.ndarray: Image exists in \"path\"\n","    list: Image size\n","    tuple: Image dimension (number of rows and columns)\n","    '''\n","    print(path)\n","    img = cv2.imread(path)  # cv2.IMREAD_GRAYSCALE)\n","    size = img.shape\n","    dimension = (size[0], size[1])\n","\n","    return img, size, dimension\n","\n","\n","def image_change_scale(img, dimension, scale=100, interpolation=cv2.INTER_LINEAR):\n","    '''Resize image to a specificall scale of original image.\n","    Parameters:\n","    img (numpy.ndarray): Original image\n","    dimension (tuple): Original image dimension\n","    scale (int): Multiply the size of the original image\n","    Returns:\n","    numpy.ndarray: Resized image\n","    '''\n","    scale /= 100\n","    new_dimension = (int(dimension[1]*scale), int(dimension[0]*scale))\n","    resized_img = cv2.resize(img, new_dimension, interpolation=interpolation)\n","\n","    return resized_img\n","\n","\n","def nearest_interpolation(image, factor):\n","    '''Nearest neighbor interpolation method to convert small image to original image\n","    Parameters:\n","    img (numpy.ndarray): Small image\n","    factor: enlargement\n","    Returns:\n","    numpy.ndarray: Resized image\n","    '''\n","    width = image.shape[0] * factor\n","    height = image.shape[1] * factor\n","\n","    new_image = np.zeros((width, height, image.shape[2]))\n","\n","    for i in range(width):\n","        for j in range(height):\n","            row = floor(i / factor)\n","            column = floor(j / factor)\n","\n","            new_image[i, j] = image[row, column]\n","\n","    return new_image\n","\n","\n","def bilinear_interpolation(image, factor):\n","    '''Bilinear interpolation method to convert small image to original image\n","    Parameters:\n","    img (numpy.ndarray): Small image\n","    dimension (tuple): resizing image dimension\n","    Returns:\n","    numpy.ndarray: Resized image\n","    '''\n","    height = image.shape[0]\n","    width = image.shape[1]\n","\n","    scale_x = (width)/(image.shape[1] * factor)\n","    scale_y = (height)/(image.shape[0] * factor)\n","\n","    new_image = np.zeros((image.shape[0] * factor, image.shape[1] * factor, image.shape[2]))\n","\n","    for k in range(3):\n","        for i in range(image.shape[0] * factor):\n","            for j in range(image.shape[1] * factor):\n","                x = (j+0.5) * (scale_x) - 0.5\n","                y = (i+0.5) * (scale_y) - 0.5\n","\n","                x_int = int(x)\n","                y_int = int(y)\n","\n","                # Prevent crossing\n","                x_int = min(x_int, width-2)\n","                y_int = min(y_int, height-2)\n","\n","                x_diff = x - x_int\n","                y_diff = y - y_int\n","\n","                a = image[y_int, x_int, k]\n","                b = image[y_int, x_int+1, k]\n","                c = image[y_int+1, x_int, k]\n","                d = image[y_int+1, x_int+1, k]\n","\n","                pixel = a*(1-x_diff)*(1-y_diff) + b*(x_diff) * \\\n","                    (1-y_diff) + c*(1-x_diff) * (y_diff) + d*x_diff*y_diff\n","\n","                new_image[i, j, k] = pixel.astype(np.uint8)\n","\n","    return new_image\n","\n","\n","def W(x):\n","    '''Weight function that return weight for each distance point\n","    Parameters:\n","    x (float): Distance from destination point\n","    Returns:\n","    float: Weight\n","    '''\n","    a = -0.5\n","    pos_x = abs(x)\n","    if -1 \u003c= abs(x) \u003c= 1:\n","        return ((a+2)*(pos_x**3)) - ((a+3)*(pos_x**2)) + 1\n","    elif 1 \u003c abs(x) \u003c 2 or -2 \u003c x \u003c -1:\n","        return ((a * (pos_x**3)) - (5*a*(pos_x**2)) + (8 * a * pos_x) - 4*a)\n","    else:\n","        return 0\n","\n","\n","def bicubic_interpolation(img, factor):\n","    '''Bicubic interpolation method to convert small size image to original size image\n","    Parameters:\n","    img (numpy.ndarray): Small image\n","    dimension (tuple): resizing image dimension\n","    Returns:\n","    numpy.ndarray: Resized image\n","    '''\n","\n","\n","    nrows = img.shape[0]*factor\n","    ncols = img.shape[1]*factor\n","\n","    output = np.zeros((nrows, ncols, img.shape[2]), np.uint8)\n","    for c in range(img.shape[2]):\n","        for i in range(nrows):\n","            for j in range(ncols):\n","                xm = (i + 0.5) * (img.shape[0]/(img.shape[0]*factor)) - 0.5\n","                ym = (j + 0.5) * (img.shape[1]/(img.shape[1]*factor)) - 0.5\n","\n","                xi = floor(xm)\n","                yi = floor(ym)\n","\n","                u = xm - xi\n","                v = ym - yi\n","\n","                # -------------- Using this make ignore some points and increase the value of black in image border\n","                # x = [(xi - 1), xi, (xi + 1), (xi + 2)]\n","                # y = [(yi - 1), yi, (yi + 1), (yi + 2)]\n","                # if ((x[0] \u003e= 0) and (x[3] \u003c img.shape[1]) and (y[0] \u003e= 0) and (y[3] \u003c img.shape[0])):\n","                #     dist_x0 = W(x[0] - xm)\n","                #     dist_x1 = W(x[1] - xm)\n","                #     dist_x2 = W(x[2] - xm)\n","                #     dist_x3 = W(x[3] - xm)\n","                #     dist_y0 = W(y[0] - ym)\n","                #     dist_y1 = W(y[1] - ym)\n","                #     dist_y2 = W(y[2] - ym)\n","                #     dist_y3 = W(y[3] - ym)\n","\n","                #     out = (img[x[0], y[0], c] * (dist_x0 * dist_y0) +\n","                #            img[x[0], y[1], c] * (dist_x0 * dist_y1) +\n","                #            img[x[0], y[2], c] * (dist_x0 * dist_y2) +\n","                #            img[x[0], y[3], c] * (dist_x0 * dist_y3) +\n","                #            img[x[1], y[0], c] * (dist_x1 * dist_y0) +\n","                #            img[x[1], y[1], c] * (dist_x1 * dist_y1) +\n","                #            img[x[1], y[2], c] * (dist_x1 * dist_y2) +\n","                #            img[x[1], y[3], c] * (dist_x1 * dist_y3) +\n","                #            img[x[2], y[0], c] * (dist_x2 * dist_y0) +\n","                #            img[x[2], y[1], c] * (dist_x2 * dist_y1) +\n","                #            img[x[2], y[2], c] * (dist_x2 * dist_y2) +\n","                #            img[x[2], y[3], c] * (dist_x2 * dist_y3) +\n","                #            img[x[3], y[0], c] * (dist_x3 * dist_y0) +\n","                #            img[x[3], y[1], c] * (dist_x3 * dist_y1) +\n","                #            img[x[3], y[2], c] * (dist_x3 * dist_y2) +\n","                #            img[x[3], y[3], c] * (dist_x3 * dist_y3))\n","\n","                #     output[i, j, c] = np.clip(out, 0, 255)\n","                # ---------------------------\n","\n","                out = 0\n","                for n in range(-1, 3):\n","                    for m in range(-1, 3):\n","                        if ((xi + n \u003c 0) or (xi + n \u003e= img.shape[1]) or (yi + m \u003c 0) or (yi + m \u003e= img.shape[0])):\n","                            continue\n","\n","                        out += (img[xi+n, yi+m, c] * (W(u - n) * W(v - m)))\n","\n","                output[i, j, c] = np.clip(out, 0, 255)\n","\n","    return output\n","\n","\n","def error_calculator(img1, img2):\n","    '''Calculate the average difference between the image pixels and returns a number.\n","    Parameters:\n","    image (numpy.ndarray): Interpolated image\n","    image (numpy.ndarray): Original image\n","    Returns:\n","    float: Average difference between two images\n","    '''\n","    return np.average(abs(np.array(img1, dtype=\"int16\") -\n","                          np.array(img2, dtype=\"int16\")))\n","\n","\n","def error_calculator_manual(img1, img2):\n","    '''Calculate the average difference between the image pixels and returns a number.\n","    Parameters:\n","    image (numpy.ndarray): Interpolated image\n","    image (numpy.ndarray): Original image\n","    Returns:\n","    float: Average difference between two images\n","    '''\n","    result = 0\n","\n","    img1 = np.array(img1, dtype=\"int16\")\n","    img2 = np.array(img2, dtype=\"int16\")\n","\n","    for i, j in zip(img1, img2):\n","        for k, l in zip(i, j):\n","            for v in range(len(k)):\n","                result += abs(k[v] - l[v])\n","\n","    if len(img1.shape) == 3:\n","        return result/(img1.shape[0] * img1.shape[1] * img1.shape[2])\n","    else:\n","        return result/(img1.shape[0] * img1.shape[1])\n","\n","\n","def show_result(images_list):\n","    '''Show result of each image action.\n","    Parameters:\n","    images list (list): List of all images\n","    Returns\n","    '''\n","    titles = list(images_list.keys())\n","    images = list(images_list.values())\n","\n","    fig, axs = plt.subplots(2, 3)\n","    fig.suptitle('25 Percent of the original size', fontsize=16)\n","\n","    axs[0, 0].set_title(titles[0])\n","    axs[0, 0].imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB))\n","\n","    axs[0, 1].set_title(titles[1])\n","    axs[0, 1].imshow(cv2.cvtColor(images[1], cv2.COLOR_BGR2RGB))\n","\n","    axs[0, 2].set_title(titles[2])\n","    axs[0, 2].imshow(cv2.cvtColor(images[2], cv2.COLOR_BGR2RGB))\n","\n","    axs[1, 0].set_title(titles[3])\n","    axs[1, 0].imshow(cv2.cvtColor(images[3], cv2.COLOR_BGR2RGB))\n","\n","    axs[1, 1].set_title(titles[4])\n","    axs[1, 1].imshow(cv2.cvtColor(images[4], cv2.COLOR_BGR2RGB))\n","\n","    axs[1, 2].set_title(titles[5])\n","    axs[1, 2].imshow(cv2.cvtColor(images[5], cv2.COLOR_BGR2RGB))\n","\n","\n","def result_comparison(error_list, color):\n","    '''Error rate comparison between the different interpolation methods.\n","    Parameters:\n","    error (list): Error rates\n","    Returns\n","    '''\n","    interpolation_methods = [\"Nearest Opencv\", \"Nearest Neighbor\",\n","                             \"Bilinear Opencv\", \"Bilinear\", \"Cubiclinear Opencv\", \"Cubiclinear\", \"Lanczos\"]\n","\n","    print(\"\\n........................Error calculate between the smalled image and the original image............................\\n\")\n","    print(f\"{interpolation_methods[0]} Error Rate: {error_list[0]}\")\n","    print(f\"{interpolation_methods[1]} Error Rate: {error_list[1]}\")\n","    print(f\"{interpolation_methods[2]} Error Rate: {error_list[2]}\")\n","    print(f\"{interpolation_methods[3]} Error Rate: {error_list[3]}\")\n","    print(f\"{interpolation_methods[4]} Error Rate: {error_list[4]}\")\n","    print(f\"{interpolation_methods[5]} Error Rate: {error_list[5]}\")\n","    print(f\"{interpolation_methods[6]} Error Rate: {error_list[6]}\\n\")\n","\n","    plt.figure()\n","    plt.bar(interpolation_methods, error_list, color=[\n","            'red', 'blue', 'purple', 'green', 'fuchsia', 'yellow', 'black'])\n","    plt.title(\"Compare result between smalled and original size image\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0h5RNJq76CPx"},"outputs":[],"source":["def main():\n","    images_list = {}\n","\n","    sample_data = ground_truth_dir + \"bird.png\"\n","    # Read Image\n","    #img, size, dimension = read_image(\"./butterfly.png\")\n","    img, size, dimension = read_image(sample_data)\n","    print(f\"Image size is: {size}\")\n","    images_list['Original Image'] = img\n","\n","    # Change Image Size\n","    scale_percent = 25  # percent of original image size\n","    resized_img = image_change_scale(img, dimension, scale_percent)\n","    print(f\"Smalled Image size is: {resized_img.shape}\")\n","    images_list['Smalled Image'] = resized_img\n","\n","    fig, axs = plt.subplots(2, 2)\n","    fig.suptitle('My Implementation', fontsize=10)\n","\n","    # Change image to original size using nearest neighbor interpolation\n","    nn_img = image_change_scale(resized_img, dimension, interpolation=cv2.INTER_NEAREST)\n","    images_list['Nearest Neighbor Interpolation'] = nn_img\n","\n","    nn_img_algo = nearest_interpolation(resized_img, 4)\n","    nn_img_algo = Image.fromarray(nn_img_algo.astype('uint8')).convert('RGB')\n","\n","    # Change image to original size using bilinear interpolation\n","    bil_img = image_change_scale(\n","        resized_img, dimension, interpolation=cv2.INTER_LINEAR)\n","    images_list['Bilinear Interpolation'] = bil_img\n","\n","    bil_img_algo = bilinear_interpolation(resized_img, 4)\n","    bil_img_algo = Image.fromarray(bil_img_algo.astype('uint8')).convert('RGB')\n","\n","    # Change image to original size using cubiclinear interpolation (4*4 pixel neighborhood)\n","    cubic_img = image_change_scale(\n","        resized_img, dimension, interpolation=cv2.INTER_CUBIC)\n","    images_list['CubicLinear Interpolation'] = cubic_img\n","\n","    # cubic_img_algo = BiCubic_interpolation(\n","    #     resized_img, dimension[0], dimension[1])\n","    cubic_img_algo = bicubic_interpolation(resized_img, 4)\n","    cubic_img_algo = Image.fromarray(\n","        cubic_img_algo.astype('uint8')).convert('RGB')\n","\n","    # Change image to original size using lanczos interpolation (8*8 pixel neighborhood)\n","    czos_img = image_change_scale(\n","        resized_img, dimension, interpolation=cv2.INTER_LANCZOS4)\n","    images_list['Lanczos Interpolation'] = czos_img\n","\n","    # error calculate between the smalled image and the original image\n","    error_list = []\n","    error_list.append(error_calculator_manual(nn_img, img))\n","    error_list.append(error_calculator_manual(nn_img_algo, img))\n","    error_list.append(error_calculator_manual(bil_img, img))\n","    error_list.append(error_calculator_manual(bil_img_algo, img))\n","    error_list.append(error_calculator_manual(cubic_img, img))\n","    error_list.append(error_calculator_manual(cubic_img_algo, img))\n","    error_list.append(error_calculator_manual(czos_img, img))\n","\n","    # Show Result\n","    show_result(images_list)\n","\n","    # Result Comparison\n","    result_comparison(error_list, \"blue\")\n","\n","    axs[0, 0].set_title(\"Original\")\n","    axs[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","\n","    axs[0, 1].set_title(\"Nearest\")\n","    axs[0, 1].imshow(cv2.cvtColor(np.array(nn_img_algo), cv2.COLOR_BGR2RGB))\n","\n","    axs[1, 0].set_title(\"Bilinear\")\n","    axs[1, 0].imshow(cv2.cvtColor(np.array(bil_img_algo), cv2.COLOR_BGR2RGB))\n","\n","    axs[1, 1].set_title(\"Bicubic\")\n","    axs[1, 1].imshow(cv2.cvtColor(np.array(cubic_img_algo), cv2.COLOR_BGR2RGB))\n","\n","    # plt.grid()\n","\n","    plt.show()\n","\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","\n","'''\n","if __name__ == \"__main__\":\n","    main()\n","'''"]},{"cell_type":"markdown","metadata":{"id":"qhzoylGZVPQK"},"source":["### Kalite Ölçümleri"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izxTr5Rm-u5H"},"outputs":[],"source":["#@title PSNR, MSE, SSIM\n","\n","def psnr(target, ref, conversion):\n","    # Assume target is RGB/BGR image\n","    target_data = target.astype(np.float32)\n","    ref_data = ref.astype(np.float32)\n","\n","\n","    #!!! CHANGED - Uncomment\n","    target_data = target\n","    ref_data = ref\n","\n","    diff = ref_data - target_data\n","    diff = diff.flatten('C')\n","\n","    rmse = np.sqrt(np.mean(diff ** 2.))\n","\n","    # Makale buna göre mi yapıyor?\n","    # 10. * torch.log10(1. / torch.mean((sr_y_tensor - hr_y_tensor) ** 2))\n","\n","    return 20 * np.log10(255. / rmse)\n","\n","def psnr_other(target, ref):\n","\n","    from math import sqrt\n","    mse = np.mean((target - ref) ** 2)\n","    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n","                  # Therefore PSNR have no importance.\n","        return 100\n","    max_pixel = 255.0\n","    psnr = 20 * np.log10(max_pixel / sqrt(mse))\n","    return psnr\n","\n","def mse(target, ref):\n","    target_data = target.astype(np.float32)\n","    ref_data = ref.astype(np.float32)\n","    err = np.sum((target_data - ref_data) ** 2)\n","\n","    err /= float(target_data.shape[0] * target_data.shape[1])\n","    return err\n","\n","from skimage.metrics import structural_similarity as ssim"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"b8JVQMb-eqOt"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/SRGAN/7-A-DISTS\n"]}],"source":["#@title A-DISTS\n","import sys\n","sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/SRGAN/7-A-DISTS')\n","%cd '/content/drive/My Drive/Colab Notebooks/SRGAN/7-A-DISTS'\n","from ADISTS_pt import *\n","from torchvision import models,transforms\n","\n","def prepare_image(image, resize=True):\n","    if resize and min(image.size)\u003e256:\n","        image = transforms.functional.resize(image,256)\n","    image = transforms.ToTensor()(image)\n","    return image.unsqueeze(0)\n","\n","def adists(image_ref,image_dist):\n","  '''\n","  image_ref,_=file_to_matrix(\"images/r0.png\",conversion=Conversions.NONE)\n","  image_dist,_=file_to_matrix(\"images/r1.png\",conversion=Conversions.NONE)\n","  # score: 0.3347\n","  '''\n","  image_ref=image_ref*255.\n","  image_dist=image_dist*255.\n","\n","  image_ref = Image.fromarray(image_ref.astype('uint8'), 'RGB')\n","  image_dist = Image.fromarray(image_dist.astype('uint8'), 'RGB')\n","\n","  ref = prepare_image(image_ref,False)\n","  dist = prepare_image(image_dist,False)\n","  assert ref.shape == dist.shape\n","\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  model = ADISTS().to(device)\n","  ref = ref.to(device)\n","  dist = dist.to(device)\n","  score = model(ref, dist)\n","\n","  return 1.0*score.item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"tuF3QuVuwZQ3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/SRGAN/7-DISTS-IQA\n"]}],"source":["#@title DISTS\n","sys.path.insert(1, '/content/drive/My Drive/Colab Notebooks/SRGAN/7-DISTS-IQA')\n","%cd '/content/drive/My Drive/Colab Notebooks/SRGAN/7-DISTS-IQA'\n","from DISTS_pytorch import *\n","from torchvision import models,transforms\n","\n","def prepare_image(image, resize=True):\n","    if resize and min(image.size)\u003e256:\n","        image = transforms.functional.resize(image,256)\n","    image = transforms.ToTensor()(image)\n","    return image.unsqueeze(0)\n","\n","def dists(image_ref,image_dist):\n","  '''\n","  image_ref and image_dist comes in [0,1] range, RGB\n","  '''\n","  image_ref=image_ref*255.\n","  image_dist=image_dist*255.\n","\n","  #image_ref,_=file_to_matrix(\"images/r0.png\",conversion=Conversions.NONE)\n","  #image_dist,_=file_to_matrix(\"images/r1.png\",conversion=Conversions.NONE)\n","  # score: 0.3347\n","\n","  image_ref = Image.fromarray(image_ref.astype('uint8'), 'RGB')\n","  image_dist = Image.fromarray(image_dist.astype('uint8'), 'RGB')\n","\n","  ref = prepare_image(image_ref,False)\n","  dist = prepare_image(image_dist,False)\n","  assert ref.shape == dist.shape\n","\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  model = DISTS().to(device)\n","  ref = ref.to(device)\n","  dist = dist.to(device)\n","  score = model(ref, dist)\n","\n","  return 1.0*score.item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"6jlK8GUGATcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cpu)\n","Requirement already satisfied: torchvision\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cpu)\n","Requirement already satisfied: numpy\u003e=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n","Requirement already satisfied: scipy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n","Requirement already satisfied: tqdm\u003e=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=0.4.0-\u003elpips) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch\u003e=0.4.0-\u003elpips) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision\u003e=0.2.1-\u003elpips) (11.0.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=0.4.0-\u003elpips) (3.0.2)\n","Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lpips\n","Successfully installed lpips-0.1.4\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:02\u003c00:00, 239MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"]}],"source":["#@title LPIPS\n","!pip install lpips\n","import lpips\n","\n","\n","import torch\n","#!pip install torchmetrics[image]\n","#_ = torch.manual_seed(123)\n","#from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","#lpips_calculate = LearnedPerceptualImagePatchSimilarity(net_type='alex')\n","#lpips_calculate = lpips.LPIPS(net='alex')\n","lpips_calculate = lpips.LPIPS(net='vgg')\n","\n","def lpips(img1,img2):\n","  lpips_output = img1.transpose(2, 0, 1).copy()\n","  lpips_output = torch.tensor(lpips_output, dtype=torch.float32).unsqueeze(0)\n","  lpips_output = lpips_output * 2 - 1\n","\n","  lpips_ground_truth = img2.transpose(2, 0, 1).copy()\n","  lpips_ground_truth = torch.tensor(lpips_ground_truth, dtype=torch.float32).unsqueeze(0)\n","  lpips_ground_truth = lpips_ground_truth * 2 - 1\n","\n","\n","  #img1=normalize_values(img1,-1,1)\n","  #img2=normalize_values(img2,-1,1)\n","  #https://torchmetrics.readthedocs.io/en/stable/image/learned_perceptual_image_patch_similarity.html\n","\n","  # LPIPS needs the images to be in the [-1, 1] range.\n","  #img1 = (torch.rand(10, 3, 100, 100) * 2) - 1\n","  #img2 = (torch.rand(10, 3, 100, 100) * 2) - 1\n","  return lpips_calculate(lpips_output, lpips_ground_truth).item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfy9lJnZVars"},"outputs":[],"source":["#Görseli kendisi ile karşılaştırınca 0 veriyor!\n","\n","\n","image_ref,_=file_to_matrix(\"images/r0-dot-dot.png\",conversion=Conversions.NONE)\n","image_dist,_=file_to_matrix(\"images/r0-dot.png\",conversion=Conversions.NONE)\n","# score: 0.3347\n","\n","\n","image_ref = Image.fromarray(image_ref.astype('uint8'), 'RGB')\n","image_dist = Image.fromarray(image_dist.astype('uint8'), 'RGB')\n","\n","ref = prepare_image(image_ref,False)\n","dist = prepare_image(image_dist,False)\n","assert ref.shape == dist.shape\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = DISTS().to(device)\n","ref = ref.to(device)\n","dist = dist.to(device)\n","score = model(ref, dist)\n","\n","score1 = score.item()\n","print(score1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk4MydekfR72"},"outputs":[],"source":["image_ref,_=file_to_matrix(\"images/r0.png\",conversion=Conversions.NONE)\n","image_dist,_=file_to_matrix(\"images/r1.png\",conversion=Conversions.NONE)\n","# score: 0.3347\n","\n","\n","image_ref = Image.fromarray(image_ref.astype('uint8'), 'RGB')\n","image_dist = Image.fromarray(image_dist.astype('uint8'), 'RGB')\n","\n","ref = prepare_image(image_ref,False)\n","dist = prepare_image(image_dist,False)\n","assert ref.shape == dist.shape\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = ADISTS().to(device)\n","ref = ref.to(device)\n","dist = dist.to(device)\n","score = model(ref, dist)\n","\n","score1 = score.item()\n","print(score1)"]},{"cell_type":"markdown","metadata":{"id":"Chl82pYeVUcs"},"source":["### Hesaplama ve Çizim Araçları"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLtt_Q-icCsd"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","#from keras.preprocessing.image import  img_to_array,load_img\n","from tensorflow.keras.utils import img_to_array,load_img\n","#from QualityMeasurements import *\n","import json\n","from datetime import date, datetime, timedelta\n","\n","class NpEncoder(json.JSONEncoder):\n","  def default(self, obj):\n","    if isinstance(obj, np.bool_):\n","      return bool(obj)\n","    if isinstance(obj, (np.floating, np.complexfloating)):\n","      return float(obj)\n","    if isinstance(obj, np.integer):\n","      return int(obj)\n","    if isinstance(obj, np.ndarray):\n","      return obj.tolist()\n","    if isinstance(obj, np.string_):\n","      return str(obj)\n","    if isinstance(obj, (datetime, date)):\n","      return obj.isoformat()\n","    if isinstance(obj, timedelta):\n","      return str(obj)\n","    return super(NpEncoder, self).default(obj)\n","\n","\n","def pretty_print(dictionary):\n","  json_str = json.dumps(dictionary, cls=NpEncoder,indent=4)\n","  print(json_str)\n","\n","\n","\n","\n","def show_stats(img):\n","  print(img.shape)\n","  print(\"MIN : {:10.5f}\".format(img.min()))\n","  print(\"MAX : {:10.5f}\".format(img.max()))\n","  print(\"MEAN: {:10.5f}\".format(img.mean()))\n","  print(\"STD : {:10.5f}\".format(img.std()))\n","\n","def save_image(filename,img):\n","  plt.imsave(filename, img)\n","  np.save(filename+\".npy\",img)\n","  print(filename+\" saved.\")\n","\n","def normalize_values(img,min,max):\n","  '''\n","  Görselde hiç BLACK yoksa maximum değer 1 olmuyor, ancak normalize edildiğinde ortaya BLACK çıkıyor.\n","  Minimum ve maximum değerleri aştığında normalizasyon yapılmalı!!!\n","  Aksi halde hatalı işlem yapılacak!!!\n","  '''\n","  img = (img-img.min()) / (img.max() - img.min())\n","  return img\n","\n","def cv2_show_image(image,factor=4,change_order=True):\n","  #INPUT: RGB\n","  #cv2 LOVES BGR\n","  #No need to change grayscale image!\n","  if (change_order):\n","    image = image[...,::-1]\n","\n","  import cv2\n","  (h,w) = image.shape[:2]\n","  image = cv2.resize(image,(w * factor,h * factor),cv2.INTER_NEAREST)\n","  cv2_imshow(image)\n","\n","def calculate_metrics(output, ground_truth,conversion):\n","    scores = {}\n","\n","    #For testing...\n","    scores[\"psnr\"]=scores[\"ssim\"]=scores[\"dists\"]=scores[\"adists\"]=scores[\"lpips\"]=0\n","\n","    #show_stats(output)\n","    #show_stats(ground_truth)\n","    #print(\"---\")\n","\n","    scores[\"psnr\"]=psnr(output*255, ground_truth*255, conversion)\n","\n","    #scores[\"mse\"]=0\n","    #scores[\"mse\"]=mse(output, ground_truth)\n","\n","    scores[\"ssim\"]=ssim(output, ground_truth, multichannel=True, channel_axis=2, data_range=1)\n","\n","    scores[\"dists\"]=dists(output, ground_truth)\n","\n","    scores[\"adists\"]=adists(output, ground_truth)\n","\n","    #scores[\"lpips\"]=0\n","    scores[\"lpips\"]=lpips(output, ground_truth)\n","\n","\n","    return scores\n","\n","\n","def show_compared_images(input_src,ground_truth_src,filename,overall_result,conversion,methods,save_diff=True,show_image=False,modified_output=False):\n","  import matplotlib.pyplot as plt\n","  import matplotlib.image as mpimg\n","\n","  # Read images\n","  #img_input = mpimg.imread(input_src+file)\n","  #img_ground_truth = mpimg.imread(ground_truth_src+file)\n","\n","  #Dönüşüm tanımlıysa önce gerçekleşiyor.\n","  img_input,color_input = file_to_matrix(input_src+filename,conversion,True,255.0)\n","  img_ground_truth,color_ground_truth = file_to_matrix(ground_truth_src+filename,conversion,True,255.0)\n","\n","\n","\n","  new_dimension=(img_ground_truth.shape[1],img_ground_truth.shape[0])\n","\n","  # The same example with just the first two images in a grid 1x2\n","  figsize_width = 28\n","  figsize_height = 8\n","  figsize = (figsize_width, figsize_height)\n","\n","  #If there's no need to show diff images, put ground truth next to others.\n","  if (save_diff):\n","    columns = 1 + len(methods)\n","    rows=2\n","  else:\n","    columns = 2 + len(methods)\n","    rows=1\n","\n","\n","  width_ratios = [4]*columns\n","  width_ratios[0]=1\n","\n","  if (show_image):\n","    fig, axes = plt.subplots(nrows=rows,ncols=columns,figsize=figsize,gridspec_kw={'width_ratios': width_ratios})\n","\n","    if (save_diff):\n","      axes[0,0].imshow(img_input)\n","      axes[0,0].set_title(\"Low Res.\")\n","    else:\n","      axes[0].imshow(img_input)\n","      axes[0].set_title(\"Low Res.\")\n","\n","  i=0\n","\n","  for output_key in methods:\n","    #print(output_key)\n","    #img_output = mpimg.imread(output_dir[output_key]+filename)\n","    if (modified_output):\n","      img_output,_ = file_to_matrix(output_modified_dir[output_key]+filename+\".npy\",conversion,remove_alpha=True,normalize_factor=255.0)\n","    else:\n","      img_output,_ = file_to_matrix(output_dir[output_key]+filename+\".npy\",conversion,remove_alpha=True,normalize_factor=255.0)\n","\n","    '''\n","    print(\"show_compared_images()\" + \"v\"*20)\n","    show_stats(img_output)\n","    show_stats(img_ground_truth)\n","    print(\"show_compared_images()\" + \"^\"*20)\n","    '''\n","\n","\n","\n","    # !!! Convert to grayscale\n","    if ((img_output.shape[0] != img_ground_truth.shape[0]) or (img_output.shape[1] != img_ground_truth.shape[1])):\n","      min_row = min(img_output.shape[0],img_ground_truth.shape[0])\n","      min_col = min(img_output.shape[1],img_ground_truth.shape[1])\n","\n","      img_ground_truth = img_ground_truth[:min_row,:min_col,:img_ground_truth.shape[2]]\n","      img_output = img_output[:min_row,:min_col,:img_output.shape[2]]\n","      print(\"\"+str(output_key)+\" shapes are inequal. NEW SHAPE: \"+str(img_ground_truth.shape))\n","\n","\n","    #Euclidean distance or only difference?\n","    #diff = (img_output**2 + img_ground_truth**2)**(1/2)\n","    diff = abs(img_output - img_ground_truth)\n","\n","\n","\n","    #FARK HESAPLAMASI HATALI! HER DÖNÜŞÜM RGB'DEN DÖNÜŞMÜYOR!\n","    diff_gray = rgb2gray(diff)\n","    #diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n","\n","    #cv2_show_image(diff_gray)\n","    #cv2_imshow(diff_gray, cmap='gray')\n","\n","    if (save_diff):\n","      difference_directory = output_diff_dir[output_key]\n","\n","      create_directory_if_not_exists(difference_directory)\n","      #cv2.imwrite(output_diff_dir+output_src.split('/')[-1], diff_gray,cmap='gray')\n","      plt.imsave(difference_directory+filename, diff_gray,cmap='gray', vmin=0, vmax=1)\n","\n","\n","    metrics = calculate_metrics(img_output, img_ground_truth, conversion)\n","    overall_result[output_key.name][conversion.name].append(metrics)\n","    #overall_result[output_key.value][conversion.value].append(metrics)\n","\n","    img_new=img_output\n","\n","    if (show_image):\n","      if (save_diff):\n","        set_axis(axes[0,1+i],output_key.name.upper(),img_new,metrics)\n","        set_axis(axes[1,1+i],\"DIFFERENCE\",diff_gray)\n","        axes[1,1+i].imshow(diff_gray, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n","      else:\n","        set_axis(axes[1+i],output_key.name.upper(),img_new,metrics)\n","\n","    i=i+1\n","\n","  if (show_image):\n","    if (save_diff):\n","      set_axis(axes[1,0],\"Ground Truth\",img_ground_truth)\n","    else:\n","      set_axis(axes[i+1],\"Ground Truth\",img_ground_truth)\n","    plt.show()\n","\n","\n","def set_axis(axis,title,img,metrics=None):\n","  axis.imshow(img)\n","  if (metrics is None):\n","    title_suffix=\"\"\n","  else:\n","    #title_suffix = \"\\nPSNR: {:10.5f}\".format(metrics[\"psnr\"])+\"\\nSSIM: {:10.5f}\".format(metrics[\"ssim\"])+\"\\nLPIPS: {:10.5f}\".format(metrics[\"lpips\"])+\"\\nDISTS: {:10.5f}\".format(metrics[\"dists\"])+\"\\nA-DISTS: {:10.5f}\".format(metrics[\"adists\"])\n","    title_suffix = \"\\nPSNR: {:10.5f}\".format(metrics[\"psnr\"])+\"\\nSSIM: {:10.5f}\".format(metrics[\"ssim\"])+\"\\nDISTS: {:10.5f}\".format(metrics[\"dists\"])+\"\\nA-DISTS: {:10.5f}\".format(metrics[\"adists\"])+\"\\nLPIPS: {:10.5f}\".format(metrics[\"lpips\"])\n","  axis.set_title(title+title_suffix)\n","\n","def interpolate(img_input, img_ground_truth, new_dimension, interpolation):\n","  img_new = cv2.resize(img_input, new_dimension, interpolation)\n","  img_psnr = psnr(img_new,img_ground_truth)\n","  img_ssim = ssim(img_new, img_ground_truth, multichannel=True)\n","\n","  return img_new, img_psnr, img_ssim\n","\n","def mean_dict_in_list(dict, key):\n","  i=0\n","  sum=0.0\n","  for data in dict:\n","    i=i+1\n","    sum = sum + data[key]\n","  return sum / i\n","\n","def mean_metric(result,list):\n","  scores={}\n","  for score in list:\n","    scores[score] = mean_dict_in_list(result,score)\n","  return scores\n","\n","def write_dataframe(selected_result):\n","  import pandas\n","\n","  print(pandas.DataFrame(selected_result).T)\n","  #mean_scores = selected_result.mean(axis=1)\n","  #col_names=[\"SCORE\",\"R-axis\",\"G-axis\",\"B-axis\"]\n","  col_names=[\"SCORE\"]\n","  row_names=[\"PSNR\",\"MSE\",\"SSIM\"]\n","  #print(mean_scores)\n","  #print(pandas.DataFrame(mean_scores, row_names, col_names))"]},{"cell_type":"markdown","metadata":{"id":"RXtGfkrgWlYK"},"source":["## Çözünürlük İyileştir"]},{"cell_type":"markdown","metadata":{"id":"xM296_ERVK-l"},"source":["### Toplu İşlem - Çözünürlük İyileştirme"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_HmJxYg9Yj2K"},"outputs":[],"source":["#@title INTERPOLATIONS\n","\n","if enhancement_mode:\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","  interpolation = None\n","  #Creating diff directories if not exists\n","  for method in Methods:\n","    dir = output_diff_dir[method]\n","    create_directory_if_not_exists(dir)\n","\n","  count=0\n","  print(\"Input dir for INTERPOLATION is \"+input_dir)\n","  for file in get_files(input_dir):\n","    count=count+1\n","    print(str(count)+\"-\"*10+\"INTERPOLATING: \"+file+\"-\"*10)\n","    img_ground_truth,_ = file_to_matrix(ground_truth_dir+file,Conversions.NONE, remove_alpha=True, normalize_factor=255.0)\n","    img,_ = file_to_matrix(input_dir+file,Conversions.NONE, remove_alpha=True, normalize_factor=255.0)\n","\n","\n","    #Çıktıya göre boyut ayarlamak yerine GİRDİYİ 4 İLE ÇARP!!!\n","    new_dimension=(img_ground_truth.shape[1],img_ground_truth.shape[0])\n","\n","\n","\n","    print(\"NEAREST NEIGHTBORHOOD\")\n","    img_new = cv2.resize(img, new_dimension, interpolation=cv2.INTER_NEAREST)\n","    show_stats(img_new)\n","    img_new = normalize_values(img_new,0,1)\n","    save_image(output_dir[Methods.INTERPOLATION_NN] +file, img_new)\n","    show_stats(img_new)\n","\n","    print(\"LINEAR\")\n","    img_new = cv2.resize(img, new_dimension, interpolation=cv2.INTER_LINEAR)\n","    show_stats(img_new)\n","    img_new = normalize_values(img_new,0,1)\n","    save_image(output_dir[Methods.INTERPOLATION_LINEAR] +file, img_new)\n","    show_stats(img_new)\n","\n","    print(\"CUBIC\")\n","    img_new = cv2.resize(img, new_dimension, interpolation=cv2.INTER_CUBIC)\n","    show_stats(img_new)\n","    img_new = normalize_values(img_new,0,1)\n","    save_image(output_dir[Methods.INTERPOLATION_CUBIC] +file, img_new)\n","    show_stats(img_new)\n","\n","    print(\"LANCZOS\")\n","    img_new = cv2.resize(img, new_dimension, interpolation=cv2.INTER_LANCZOS4)\n","    show_stats(img_new)\n","    img_new = normalize_values(img_new,0,1)\n","    save_image(output_dir[Methods.INTERPOLATION_LANCZOS] +file, img_new)\n","    show_stats(img_new)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_so2gR70iH9g"},"outputs":[],"source":["#@title SRGAN\n","if enhancement_mode:\n","  #Yeni görsel üretmek için output klasörünün temizlenmesi gerekiyor, eski resmin üstüne yazmıyor.\n","  os.chdir(base_dir[Methods.SRGAN])\n","\n","  !python \"{base_dir[Methods.SRGAN]}validate.py\" -lr_dir \"{input_dir}\" -hr_dir \"{ground_truth_dir}\" -sr_dir \"{output_dir[Methods.SRGAN]}\"\n","\n","  '''\n","  for file in filenames:\n","    print(\"File is \"+file)\n","    !python \"{base_dir[Methods.SRGAN]}validate.py\" -file_name \"{input_dir}{file}\"\n","  '''\n","\n","  #config.py içinde belirtlen datasete göre yapar\n","  #!python validate.py"]},{"cell_type":"markdown","metadata":{"id":"W3ICaHTH24vP"},"source":["#### Real-ESRGAN"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nBRBnXS23ZyH"},"outputs":[],"source":["#@title Gereksinimleri Yükleme\n","if enhancement_mode:\n","  #Real-ESRGAN\n","  # Install basicsr - https://github.com/xinntao/BasicSR\n","  # We use BasicSR for both training and inference\n","  !pip install basicsr\n","  # facexlib and gfpgan are for face enhancement\n","  !pip install facexlib\n","  !pip install gfpgan\n","  !pip install ffmpeg-python\n","  !pip install -r requirements.txt\n","  !python setup.py develop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"111d0hqu2DZw"},"outputs":[],"source":["#@title Çalıştırma\n","if enhancement_mode:\n","  os.chdir(base_dir[Methods.REAL_ESRGAN])\n","\n","  filenames=get_files(input_dir)\n","\n","  for file in filenames:\n","    print(\"File is \"+file)\n","    !python \"{base_dir[Methods.REAL_ESRGAN]}inference_realesrgan.py\" -n RealESRGAN_x4plus -o \"{output_dir[Methods.REAL_ESRGAN]}\" --suffix \"\" -i \"{input_dir}{file}\""]},{"cell_type":"markdown","metadata":{"id":"Pq8amc9lI87C"},"source":["#### Swin2SR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ux2cbPhzOGJf"},"outputs":[],"source":["#@title Gereksinimleri Yükleme\n","if enhancement_mode:\n","  !pip install timm\n","  !python -c \"from timm import list_models; print(list_models(pretrained=True)[:5])\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2b6-XETIyQ7"},"outputs":[],"source":["#@title Çalıştırma\n","if enhancement_mode:\n","  #Yeni görsel üretmek için output klasörünün temizlenmesi gerekiyor, eski resmin üstüne yazmıyor.\n","  os.chdir(base_dir[Methods.SWIN2SR])\n","  !python \"{base_dir[Methods.SWIN2SR]}main_test_swin2sr.py\" --task classical_sr --scale 4 --training_patch_size 64 --model_path model_zoo/swin2sr/Swin2SR_ClassicalSR_X4_64.pth --folder_lq \"{input_dir}\" --folder_gt \"{ground_truth_dir}\" --save_img_only"]},{"cell_type":"markdown","metadata":{"id":"_fSYyulZW0VW"},"source":["### İyileştirilmiş Görsel İnceleme\n","\n","input olarak kullanılacak görseli keskinleştir. Ancak bunu yaparken RGB'nin tamamını düşünmek gerekecektir.\n","\n","Aşağıdaki kaynaklar keskinleştirme konusunda fikir verecektir.\n","\n","https://www.analyticsvidhya.com/blog/2021/08/sharpening-an-image-using-opencv-library-in-python/\n","\n","https://stackoverflow.com/questions/60894593/how-to-sharpen-the-edges-in-opencv-python\n","\n","https://towardsdatascience.com/image-processing-with-python-blurring-and-sharpening-for-beginners-3bcebec0583a\n","\n","Daha iyi bir sharpen kernel mümkün mü? Yeni bir sharpen kernel geliştirilemez mi? Kenarları daha iyi ortaya çıkartan ancak yakın dokular üzerinde çok oynama yapmayan bir çözüm gerekiyor. Böylece doku detayları yakalanabilir.\n","\n","**En uygun KERNEL işlemleri için fark görselleri üzerindeki mesafeye bakılması gerekecek. En uygununu seçmek için de tüm veriseti üzerinde çalıştırıp ortalama sonuçlar karşılaştırılmalı.**\n","\n","SREdgeNet İNCELENMELİ"]},{"cell_type":"markdown","metadata":{"id":"Kws6XhkzWjSv"},"source":["#### Edge Detection - old"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPy67FGWYmnd"},"outputs":[],"source":["if (edge_detection_mode):\n","\n","  # Modifiye edilmiş dosya kaydedilmeli ve sonra yolu gönderilmeli\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","\n","  file_name = \"butterfly.png\"\n","  file_path = \"/content/drive/MyDrive/Colab Notebooks/SRGAN/dataset/Set5/LRbicx4/\"\n","  modified_file_path = \"/content/drive/MyDrive/Colab Notebooks/SRGAN/temp.png\"\n","\n","  image = cv2.imread(file_path+file_name, cv2.IMREAD_UNCHANGED).astype(np.float32)\n","  cv2_imshow(image)\n","  #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","\n","  kernel = []\n","\n","  #Sobel–Feldman operator, Alternative, https://en.wikipedia.org/wiki/Sobel_operator\n","  kernel.append(\n","                np.array([[3,    0,    -3],\n","                          [10,    0,    -10],\n","                          [3,    0,    -3]])\n","  )\n","\n","  #Sharpen\n","  kernel.append(\n","                np.array([[0, -1, 0],\n","                          [-1, 5,-1],\n","                          [0, -1, 0]])/8\n","  )\n","\n","  kernel.append(\n","                np.array([[1,0,-1],\n","                          [2,0,-2],\n","                          [1,0,-1]])/10\n","  )\n","\n","  for i in range(len(kernel)):\n","    image = cv2.filter2D(src=image, ddepth=-1, kernel=kernel[i])\n","    print(\"Kernel: \"+str(i))\n","    cv2_imshow(image)\n","\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    cv2_imshow(gray_image)\n","\n","    gray_image = cv2.filter2D(src=gray_image, ddepth=-1, kernel=kernel[i])\n","    cv2_imshow(gray_image)\n","\n","\n","    gray_image = cv2.filter2D(src=gray_image, ddepth=-1, kernel=kernel[i])\n","    cv2_imshow(gray_image)\n","\n","\n","\n","\n","\n","    continue\n","    result = cv2.imwrite(modified_file_path, image)\n","\n","    if result:\n","      modified_file_path_space_out = modified_file_path.replace(\" \",\"@\")\n","      !python validate.py -file_name \"butterfly.png\" -modified_file_path $modified_file_path_space_out\n","\n","      show_compared_images(input_dir+\"/\"+file_name,output_dir+\"/\"+file_name,ground_truth_dir+\"/\"+file_name)\n","    else:\n","      print(\"File write error\")\n","\n","    # Üzerinde oynanmış görseli kabul edecek bir yapı düşünmek gerekecek, dosya isminin yanında bu veri de gönderilmeli\n","    # Sadece STRING kabul ediliyor. Dosya sistemine yazmak dışında bir seçenek yok gibi..."]},{"cell_type":"markdown","metadata":{"id":"FdFWKNeDXYYp"},"source":["#### Sharpen Kernel and Prewitt - BAŞARILI GÖRÜNÜYOR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgYxBMoD9xkm"},"outputs":[],"source":["def pixel_extend(img,x,y,value,extend_size):\n","  for i in range(extend_size):\n","    for j in range(extend_size):\n","      img[i+x][j+y]=value\n","\n","\n","def apply_local_filter(img,ref_img,x,y,filter,extend_size):\n","  if (x - filter.shape[0]/2 \u003c 0 or y - filter.shape[1]/2 \u003c 0):\n","    return\n","  if (x + filter.shape[0]/2 + extend_size \u003e= img.shape[0]-2 or y + filter.shape[1]/2 + extend_size \u003e= img.shape[1]-2):\n","    return\n","\n","  filter_width = filter.shape[0]\n","  filter_height =  filter.shape[1]\n","\n","  start_x = x - int(filter_width/2)\n","  start_y = y - int(filter_height/2)\n","\n","  end_x = x + filter_width - 1\n","  end_y = y + filter_height - 1\n","  for plus_i in range(extend_size):\n","\n","    for i in range(start_x + plus_i,end_x + plus_i):\n","      filter_i=0\n","      for plus_j in range(extend_size):\n","        filter_j=0\n","        for j in range(start_y + plus_j,end_y + plus_j):\n","          #print(str(i)+\"-\"+str(j) + \"FILTER\" + str(filter_i)+\"-\"+str(filter_j))\n","          img[i][j] = ref_img[i][j] * filter[filter_i][filter_j]\n","          filter_j=filter_j+1\n","      filter_i=filter_i+1\n","\n","#ref_img=img.copy()\n","#apply_local_filter(img,ref_img,50,50,filter,4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtMHtukfXVfx"},"outputs":[],"source":["if (edge_detection_mode):\n","\n","  # 1-Grayscale\n","  # 2-Sharpen kernel 1. geçiş\n","  # 3-Sharpen kernel 2. geçiş\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","\n","  show_images=True\n","\n","  create_directory_if_not_exists(input_edge_dir)\n","  create_directory_if_not_exists(output_modified_dir[Methods.SRGAN])\n","\n","  file_names = [\"butterfly.png\"]\n","\n","  #If working with only 1 file, comment the below line\n","  #file_names = get_files(input_dir)\n","\n","\n","  for file_name in file_names:\n","    print(file_name)\n","\n","    modified_file_path = input_edge_dir+file_name\n","\n","    print(input_dir+file_name)\n","\n","    image,_ = file_to_matrix(input_dir+file_name,Conversions.NONE)\n","    #image = image[...,::-1]\n","    #cv2_show_image(image)\n","\n","\n","    #image = cv2.imread(input_dir+file_name, cv2.IMREAD_UNCHANGED).astype(np.float32)\n","    if show_images:\n","      print(\"4x zoom\")\n","      cv2_show_image(image)\n","\n","    #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","\n","    kernel = []\n","    #Sharpen\n","    kernel.append(\n","                  np.array([[0, -1, 0],\n","                            [-1, 5,-1],\n","                            [0, -1, 0]])/6\n","    )\n","\n","\n","    kernel.append(\n","                  np.array([[0, -1, 0],\n","                            [-1, 5,-1],\n","                            [0, -1, 0]])/6\n","    )\n","\n","    print(np.array(kernel).shape)\n","\n","    #print(\"Kernel pass on original low-res image:\")\n","    #image = cv2.filter2D(src=image, ddepth=-1, kernel=kernel[i])\n","    #cv2_imshow(image)\n","\n","\n","    #BGR2GRAY OR RGB2GRAY?\n","    #gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","    if show_images:\n","      print(\"Grayscale\")\n","      cv2_show_image(gray_image,4,False)\n","\n","    for i in range(len(kernel)):\n","      gray_image = cv2.filter2D(src=gray_image, ddepth=-1, kernel=kernel[i])\n","      if show_images:\n","        print(\"After pass of kernel: \"+str(i))\n","        cv2_show_image(gray_image,4,False)\n","\n","    #Kaldırılacak\n","    #(thresh, black_white_image) = cv2.threshold(gray_image, 5, 255, cv2.THRESH_BINARY)\n","\n","    #print(\"Black and White\")\n","    #cv2_show_image(black_white_image)\n","\n","    #result = cv2.imwrite(modified_file_path, black_white_image)\n","\n","    '''\n","    #CANNY\n","    gray_image = np.uint8(gray_image)\n","    gray_image = cv2.Canny(gray_image,100,200)\n","    gray_image = cv2.Canny(image=gray_image,threshold1=100,threshold2=200)\n","    cv2_show_image(gray_image)\n","    '''\n","\n","\n","\n","    #PREWITT YİNE KULLANILSIN\n","    kernelx = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])*0.5\n","    kernely = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])*0.5\n","    img_prewittx = cv2.filter2D(gray_image, -1, kernelx)\n","    img_prewitty = cv2.filter2D(gray_image, -1, kernely)\n","\n","    img_prewitt = img_prewittx + img_prewitty\n","\n","\n","    print(\"Prewitt Applied #1\")\n","    cv2_show_image(img_prewitt,4,False)\n","\n","\n","    result = cv2.imwrite(modified_file_path, img_prewitt)\n","    #print(img_prewitt.round(decimals=0))\n","\n","    img = img_prewitt\n","    #img = gray_image\n","\n","\n","    '''\n","    #Example of bad idea :)\n","    #0 and 255 polarizing according to mean-std\n","    threshold = img.mean() + img.std()\n","    for i in range(img.shape[0]):\n","      for j in range(img.shape[1]):\n","        if (img[i][j]\u003ethreshold):\n","          img[i][j]=255\n","        else:\n","          img[i][j]=0\n","    print(\"Trimmed values out of threshold\")\n","    cv2_show_image(img,4,False)\n","    '''\n","\n","    #Dosyaya yazılan ve dosyadan okunan görseller farklı görünüyor!!!\n","\n","    #img = cv2.imread(modified_file_path)\n","    #cv2_show_image(img)\n","    #print(img)\n","    #img.describe()\n","    #print(\"Reading from file after saving:\")\n","    show_stats(img)\n","\n","\n","    #ROI'deki pikselin fark değerine göre daha güçlü bir şekilde keskinleştirme uygulanabilir.\n","    #Yüksek fark = yüksek keskinleştirme\n","\n","    '''\n","\n","    filter=np.array([[1, 1.1, 1],\n","                    [1.1, 1.5, 1.1],\n","                    [1, 1.1, 1]])\n","    extend_size=1\n","\n","    enhanced_file=output_dir[Methods.SRGAN]+file_name\n","    print(enhanced_file)\n","    enhanced_img,_=file_to_matrix(enhanced_file,Conversions.NONE)\n","    enhanced_ref_img = enhanced_img.copy()\n","    enhanced_spot_img = enhanced_img.copy()\n","    for i in range(img.shape[0]):\n","      for j in range(img.shape[1]):\n","        if (img[i][j]\u003e0):\n","          value=[255.,0.,0,]\n","          pixel_extend(enhanced_spot_img,i*4,j*4,value,4)\n","          apply_local_filter(enhanced_img,enhanced_ref_img,i*4,j*4,filter,extend_size)\n","          #enhanced_img[i][j]=value\n","\n","    cv2_show_image(enhanced_spot_img)\n","    cv2_show_image(enhanced_img)\n","    cv2_show_image(enhanced_ref_img)\n","\n","    show_stats(enhanced_img)\n","    enhanced_img = normalize_values(enhanced_img,0,1)\n","    save_image(output_modified_dir[Methods.SRGAN]+file_name, enhanced_img)\n","\n","    '''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QaqxsJFr9rM"},"outputs":[],"source":["#@title Edge Detectors (Gradiant Magnitude?)\n","if (edge_detection_mode):\n","\n","  # https://gist.github.com/rahit/c078cabc0a48f2570028bff397a9e154\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","  import numpy as np\n","\n","  create_directory_if_not_exists(input_edge_dir)\n","  file_name = \"butterfly.png\"\n","\n","\n","  file_names=get_files(input_dir)\n","  for file_name in file_names:\n","    print(file_name)\n","\n","    modified_file_path = input_edge_dir+file_name\n","\n","\n","\n","\n","\n","    img = cv2.imread(input_dir+file_name)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    img_gaussian = cv2.GaussianBlur(gray,(3,3),0)\n","\n","    #canny\n","    img_canny = cv2.Canny(img,100,200)\n","\n","    #sobel\n","    img_sobelx = cv2.Sobel(img_gaussian,cv2.CV_8U,1,0,ksize=5)\n","    img_sobely = cv2.Sobel(img_gaussian,cv2.CV_8U,0,1,ksize=5)\n","    img_sobel = img_sobelx + img_sobely\n","\n","\n","    #prewitt\n","    kernelx = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n","    kernely = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])\n","    img_prewittx = cv2.filter2D(img_gaussian, -1, kernelx)\n","    img_prewitty = cv2.filter2D(img_gaussian, -1, kernely)\n","\n","\n","    print(\"Original Image\")\n","    cv2_imshow(img)\n","\n","    print(\"Canny\")\n","    cv2_imshow(img_canny)\n","\n","    #print(\"Sobel X\")\n","    #cv2_imshow(img_sobelx)\n","\n","    #print(\"Sobel Y\")\n","    #cv2_imshow(img_sobely)\n","\n","    print(\"Sobel\")\n","    cv2_imshow(img_sobel)\n","\n","    #print(\"Prewitt X\")\n","    #cv2_imshow(img_prewittx)\n","\n","    #print(\"Prewitt Y\")\n","    #cv2_imshow(img_prewitty)\n","\n","    print(\"Prewitt\")\n","    cv2_imshow(img_prewittx + img_prewitty)\n","\n","\n","    #result = cv2.imwrite(modified_file_path, black_white_image)\n","\n","\n","  if False:\n","    if result:\n","      modified_file_path_space_out = modified_file_path.replace(\" \",\"@\")\n","      !python validate.py -file_name $file_name -modified_file_path $modified_file_path_space_out\n","\n","      show_compared_images(input_dir+\"/\"+file_name,output_dir+\"/\"+file_name,ground_truth_dir+\"/\"+file_name)\n","    else:\n","      print(\"File write error\")\n","\n","  # Görseller çift kutuplu bir yapıya dönüştürülmeli"]},{"cell_type":"markdown","metadata":{"id":"4YCZByuPDFK8"},"source":["#### Edge PSNR ve SSIM Hesaplama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3u8MEXFmDN5C"},"outputs":[],"source":["if (edge_detection_mode):\n","\n","  #ŞİMDİLİK BU KISIM ÖNEMLİ DEĞİL, EXTRACTED EDGE YETERİNCE İYİ SONUÇ VERMEZSE BAKILABİLİR.\n","\n","  input_edge_files = get_files(input_edge_dir)\n","\n","  overall_result=[]\n","\n","  def edge_assessment(output_diff_dir):\n","    overall_result=[]\n","    for file_name in input_edge_files:\n","      input_file_name = input_edge_dir + file_name\n","      output_file_name = output_diff_dir + file_name\n","      #ground_truth_file_name = ground_truth_dir + file_name\n","\n","      input_matrix = file_to_matrix(input_file_name,Conversion.NONE,True,255.0)\n","      output_matrix = file_to_matrix(output_file_name,Conversion.NONE,True,255.0)\n","      scores = compare_images(cv2.resize(input_matrix, (input_matrix.shape[1]*4,input_matrix.shape[0]*4), cv2.INTER_NEAREST),output_matrix)\n","      overall_result.append(scores)\n","\n","  #edge_assessment(output_diff_dir[\"srgan\"])\n","\n","  print(\"-\"*10 + \" MEAN OF EDGE SCORES \" + \"-\"*10)\n","\n","  import pandas\n","\n","  overall_result = np.array(overall_result)\n","\n","  mean_scores = overall_result.mean(axis=0)\n","  col_names=[\"SCORE\",\"R-axis\",\"G-axis\",\"B-axis\"]\n","  row_names=[\"PSNR\",\"MSE\",\"SSIM\"]\n","  print(pandas.DataFrame(mean_scores, row_names, col_names))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VuiC-6EStOTh"},"source":["## Sonuçları Göster"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AojqReCn4lJN"},"outputs":[],"source":["#DISTS hack\n","%cd '/content/drive/My Drive/Colab Notebooks/SRGAN/7-DISTS-IQA'\n","\n","#os.chdir(base_dir[Methods.SWIN2SR])\n","\n","if (show_comparison_results):\n","  import numpy as np\n","\n","\n","  filenames = get_files(input_dir)\n","  overall_result={}\n","\n","  calculate_all_dataset=True # If False, only show_images_list will be calculated.\n","  show_images_list=[\"baby.png\",\"bird.png\",\"butterfly.png\",\"head.png\",\"woman.png\",\"ppt3.png\",\"236037.png\",\"img_008.png\"]\n","  show_images_list=[\"baboon.png\",\"lenna.png\"]\n","\n","  show_all_images=False # If True, all images will be shown, else only specified images will be shown\n","  show_process_results=False\n","  save_diff=True # If False, GT will be placed at the end of the image list\n","\n","  methods_list = Methods\n","  conversions_list = Conversions\n","\n","\n","\n","  #methods_list = [Methods.INTERPOLATION_CUBIC]\n","  #conversions_list = [Conversions.NONE]\n","\n","\n","  for method in methods_list:\n","    overall_result[method.name]={}\n","    for conversion in conversions_list:\n","      overall_result[method.name][conversion.name]=[]\n","\n","  count=0\n","  for file in filenames:\n","    if (file in show_images_list):\n","      show_image=True\n","    else:\n","      if (not calculate_all_dataset):\n","        continue\n","      show_image=show_all_images\n","    count=count+1\n","    print(\"File: #\"+str(count)+\": \"+file)\n","    for conversion in conversions_list:\n","      print(\"CONVERSION: \"+conversion.name)\n","      show_compared_images(input_dir, ground_truth_dir,file,overall_result, conversion, methods=methods_list, save_diff=save_diff, show_image=show_image,modified_output=modified_results)\n","\n","\n","  if (show_process_results):\n","    print(\"-\"*10 + \" RESULTS OF EACH PROCESS\" + \"-\"*10)\n","    pretty_print(overall_result)\n","\n","if (show_comparison_results):\n","\n","  print(\"-\"*10 + \" MEAN OF OVERALL SCORES\" + \"-\"*10)\n","\n","  overall_mean={}\n","  scores_list = [\"psnr\",\"ssim\",\"dists\",\"adists\",\"lpips\"]\n","  for method in methods_list:\n","    for conversion in conversions_list:\n","      name = method.name.ljust(25)+conversion.name.ljust(10)\n","      overall_mean[name]={}\n","\n","      overall_mean[name]=mean_metric(overall_result[method.name][conversion.name],scores_list)\n","\n","  #pretty_print(overall_mean)\n","  write_dataframe(overall_mean)\n"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["nY5Yfr_0xtRo","0xnA5rFHbBRx","T3AlYnnu0t4E","RXtGfkrgWlYK","W3ICaHTH24vP","_fSYyulZW0VW","Kws6XhkzWjSv"],"gpuType":"V28","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}